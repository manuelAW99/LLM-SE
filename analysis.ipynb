{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a65998",
   "metadata": {},
   "source": [
    "# Power Consumption Analysis\n",
    "\n",
    "This notebook analyzes power consumption metrics from CSV data and correlates them with LLM benchmark request timings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e61c98",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1712a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21c957",
   "metadata": {},
   "source": [
    "## Load and Prepare CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open CSV file and load data into DataFrame\n",
    "data = pd.read_csv('.\\\\benchmark_results\\\\local.CSV', encoding='latin-1', decimal='.', thousands=',')\n",
    "# Remove leading/trailing whitespace and special characters (like BOM) from column names\n",
    "data.columns = data.columns.str.strip().str.replace('\\ufeff', '', regex=False)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(data.head())\n",
    "print(f\"\\nColumn names: {data.columns[:5].tolist()}\")  # Print first 5 column names to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c31a4",
   "metadata": {},
   "source": [
    "## Filter Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns to keep only the relevant metrics\n",
    "columns_to_keep = [\n",
    "    'Potenza totale CPU [W]',\n",
    "    \"CPU Package Power [W]\",\n",
    "    \n",
    "    'Potenza Core IA [W]',\n",
    "    \"IA Cores Power [W]\",\n",
    "    \n",
    "    'VR VCC Corrente (SVID IOUT) [A]',\n",
    "    \"VR VCC Current (SVID IOUT) [A]\"\n",
    "    \n",
    "    'GPU Potenza [W]',\n",
    "    \"GPU Power [W]\",\n",
    "    \n",
    "    'IGPU Potenza [W]',\n",
    "    \"IGPU Power [W]\"\n",
    "    \n",
    "    'Potenza DRAM totale [W]',\n",
    "    'Total DRAM Power [W]',\n",
    "    \n",
    "    'Consumo energetico resto del chip [W]',\n",
    "    'Rest-of-Chip Power [W]'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the DataFrame\n",
    "existing_columns = [col for col in columns_to_keep if col in data.columns]\n",
    "data_filtered = data[existing_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0a00c",
   "metadata": {},
   "source": [
    "## Convert to Numeric and Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb711dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numeric values, handling errors\n",
    "for col in data_filtered.columns:\n",
    "    data_filtered[col] = pd.to_numeric(data_filtered[col], errors='coerce')\n",
    "\n",
    "# Replace NaN values with 0\n",
    "data_filtered = data_filtered.fillna(0)\n",
    "\n",
    "print(data_filtered.head())\n",
    "print(data_filtered.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f1c45",
   "metadata": {},
   "source": [
    "## Add Timestamp Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the first two time-related columns to datetime\n",
    "# Combine Date and Time columns to create a datetime column\n",
    "data_filtered['timestamp'] = pd.to_datetime(\n",
    "    data['Date'] + ' ' + data['Time'],\n",
    "    format='%d.%m.%Y %H:%M:%S.%f',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"\\nCSV time range (corrected): {data_filtered['timestamp'].min()} to {data_filtered['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e5b4c",
   "metadata": {},
   "source": [
    "## Load JSON Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce546c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON benchmark data\n",
    "json_file = '.\\\\benchmark_results\\\\benchmark_qwen_qwen3-4b-thinking-2507_20260126_004413.json'\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "# Extract request data and timestamps from JSON\n",
    "request_events = []\n",
    "if 'results' in benchmark_data:\n",
    "    for entry in benchmark_data['results']:\n",
    "        request_events.append({\n",
    "            'timestamp_send': pd.to_datetime(entry['timestamp_send']),\n",
    "            'timestamp_response': pd.to_datetime(entry['timestamp_response']),\n",
    "            'total_tokens': entry.get('total_tokens', 0),\n",
    "            'elapsed_time': entry.get('elapsed_time_seconds', 0),\n",
    "            'size_category': entry.get('size_category', 'unknown')\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "request_df = pd.DataFrame(request_events)\n",
    "\n",
    "if len(request_df) > 0:\n",
    "    print(f\"\\nJSON time range: {request_df['timestamp_send'].min()} to {request_df['timestamp_send'].max()}\")\n",
    "    print(f\"Total requests: {len(request_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48a6ec",
   "metadata": {},
   "source": [
    "## Filter CSV Data to Match Request Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(request_df) > 0:\n",
    "    # Get the first timestamp_send and last timestamp_response from JSON\n",
    "    first_json_timestamp = request_df['timestamp_send'].min()\n",
    "    last_json_timestamp = request_df['timestamp_response'].max()\n",
    "    print(f\"\\nFirst timestamp_send in JSON: {first_json_timestamp}\")\n",
    "    print(f\"Last timestamp_response in JSON: {last_json_timestamp}\")\n",
    "    \n",
    "    # Filter CSV data to show only from first request send to last response\n",
    "    data_filtered = data_filtered[\n",
    "        (data_filtered['timestamp'] >= first_json_timestamp) & \n",
    "        (data_filtered['timestamp'] <= last_json_timestamp)\n",
    "    ].copy()\n",
    "    print(f\"CSV data filtered to {len(data_filtered)} rows (from first request to last response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa156c0",
   "metadata": {},
   "source": [
    "## Match Requests with CSV Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(request_df) > 0:\n",
    "    # For each request, find the closest CSV measurement\n",
    "    request_df['csv_index'] = request_df['timestamp_send'].apply(\n",
    "        lambda req_time: (data_filtered['timestamp'] - req_time).abs().idxmin() \n",
    "        if pd.notna(data_filtered['timestamp']).any() else None\n",
    "    )\n",
    "    \n",
    "    # Get the closest timestamp for each request\n",
    "    request_df['closest_csv_time'] = request_df['csv_index'].apply(\n",
    "        lambda idx: data_filtered.loc[idx, 'timestamp'] if idx is not None else None\n",
    "    )\n",
    "    \n",
    "    request_df['time_diff_seconds'] = (request_df['timestamp_send'] - request_df['closest_csv_time']).dt.total_seconds()\n",
    "    \n",
    "    print(f\"\\nTime difference statistics (seconds):\")\n",
    "    print(request_df['time_diff_seconds'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4535401",
   "metadata": {},
   "source": [
    "## Visualize Power Metrics with Request Time Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each category\n",
    "category_colors = {\n",
    "    'short': 'green',\n",
    "    'medium': 'skyblue',\n",
    "    'long': 'red',\n",
    "}\n",
    "\n",
    "# Plot power metrics with request time bands\n",
    "for column in existing_columns:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot power data vs timestamp\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(data_filtered['timestamp'], data_filtered[column], label=column, color='blue', linewidth=1.5)\n",
    "    ax1.set_ylabel('Power (W)', color='blue', fontsize=12)\n",
    "    ax1.set_xlabel('Time', fontsize=12)\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add time bands for each request\n",
    "    if len(request_df) > 0:\n",
    "        # Track which categories have been added to legend\n",
    "        added_to_legend = set()\n",
    "        \n",
    "        for idx, row in request_df.iterrows():\n",
    "            category = row['size_category']\n",
    "            color = category_colors.get(category, 'lightgray')\n",
    "            label = f'{category.capitalize()} Request' if category not in added_to_legend else None\n",
    "            \n",
    "            # Draw vertical span from timestamp_send to timestamp_response\n",
    "            ax1.axvspan(row['timestamp_send'], row['timestamp_response'], \n",
    "                       alpha=0.3, color=color, label=label)\n",
    "            \n",
    "            if label:\n",
    "                added_to_legend.add(category)\n",
    "    \n",
    "    plt.title(f'{column} with LLM Request Time Periods', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='lower left')\n",
    "    \n",
    "    # Format x-axis for better readability\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e914c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive statistics calculation function\n",
    "def calculate_power_statistics(data, requests, power_columns=None, group_by=None, filter_by=None):\n",
    "    \"\"\"\n",
    "    Calculate power consumption statistics with flexible grouping and filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with power metrics and timestamps\n",
    "    - requests: DataFrame with request information\n",
    "    - power_columns: List of power columns to analyze (default: all existing_columns)\n",
    "    - group_by: List of grouping criteria ('model', 'size_category', 'timestamp_hour')\n",
    "    - filter_by: Dict with column name and values to filter (e.g., {'size_category': ['short', 'medium']})\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with statistics for each group\n",
    "    \"\"\"\n",
    "    \n",
    "    if power_columns is None:\n",
    "        power_columns = existing_columns\n",
    "    \n",
    "    if group_by is None:\n",
    "        group_by = ['size_category']\n",
    "    \n",
    "    # Add csv_index and power data to requests\n",
    "    working_df = requests.copy()\n",
    "    for col in power_columns:\n",
    "        working_df[col] = working_df['csv_index'].apply(\n",
    "            lambda idx: data.loc[idx, col] if idx is not None and col in data.columns else 0\n",
    "        )\n",
    "    \n",
    "    # Calculate energy consumption during each request\n",
    "    working_df['duration_seconds'] = (working_df['timestamp_response'] - working_df['timestamp_send']).dt.total_seconds()\n",
    "    for col in power_columns:\n",
    "        working_df[f'{col}_energy_Wh'] = (working_df[col] * working_df['duration_seconds']) / 3600\n",
    "    \n",
    "    # Apply filters if provided\n",
    "    if filter_by:\n",
    "        for col, values in filter_by.items():\n",
    "            working_df = working_df[working_df[col].isin(values)]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results = {}\n",
    "    \n",
    "    # Group by specified criteria\n",
    "    if group_by:\n",
    "        grouped = working_df.groupby(group_by)\n",
    "        for group_name, group_data in grouped:\n",
    "            group_key = str(group_name)\n",
    "            results[group_key] = {}\n",
    "            \n",
    "            for col in power_columns:\n",
    "                results[group_key][col] = {\n",
    "                    'avg_power_W': group_data[col].mean(),\n",
    "                    'total_energy_Wh': group_data[f'{col}_energy_Wh'].sum(),\n",
    "                    'max_power_W': group_data[col].max(),\n",
    "                    'min_power_W': group_data[col].min(),\n",
    "                    'std_dev': group_data[col].std(),\n",
    "                    'count': len(group_data)\n",
    "                }\n",
    "    else:\n",
    "        # Overall statistics\n",
    "        results['overall'] = {}\n",
    "        for col in power_columns:\n",
    "            results['overall'][col] = {\n",
    "                'avg_power_W': working_df[col].mean(),\n",
    "                'total_energy_Wh': working_df[f'{col}_energy_Wh'].sum(),\n",
    "                'max_power_W': working_df[col].max(),\n",
    "                'min_power_W': working_df[col].min(),\n",
    "                'std_dev': working_df[col].std(),\n",
    "                'count': len(working_df)\n",
    "            }\n",
    "    \n",
    "    return results, working_df\n",
    "\n",
    "# Calculate correlation between power consumption and request characteristics\n",
    "def calculate_correlations(working_df, power_columns=None):\n",
    "    \"\"\"\n",
    "    Calculate correlations between power metrics and request characteristics.\n",
    "    \"\"\"\n",
    "    if power_columns is None:\n",
    "        power_columns = existing_columns\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    for col in power_columns:\n",
    "        correlations[col] = {\n",
    "            'vs_total_tokens': working_df[col].corr(working_df['total_tokens']),\n",
    "            'vs_elapsed_time': working_df[col].corr(working_df['elapsed_time']),\n",
    "            'vs_duration_seconds': working_df[col].corr(working_df['duration_seconds'])\n",
    "        }\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Display results in a readable format\n",
    "def display_statistics(stats_dict, title=\"Power Consumption Statistics\"):\n",
    "    \"\"\"Display statistics in a formatted table.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for group, metrics in stats_dict.items():\n",
    "        print(f\"\\n{group}:\")\n",
    "        print(\"-\" * 80)\n",
    "        for power_col, stats in metrics.items():\n",
    "            print(f\"\\n  {power_col}:\")\n",
    "            for metric, value in stats.items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"    {metric}: {value:.2f}\")\n",
    "                else:\n",
    "                    print(f\"    {metric}: {value}\")\n",
    "\n",
    "# Example usage - Calculate statistics grouped by size_category\n",
    "stats_by_category, df_with_energy = calculate_power_statistics(\n",
    "    data_filtered, \n",
    "    request_df,\n",
    "    power_columns=existing_columns,\n",
    "    group_by=['size_category']\n",
    ")\n",
    "\n",
    "display_statistics(stats_by_category, \"Statistics by Request Size Category\")\n",
    "\n",
    "# Calculate overall correlations\n",
    "correlations = calculate_correlations(df_with_energy, existing_columns)\n",
    "print(\"\\n\\nCorrelations with Request Characteristics:\")\n",
    "for col, corr_dict in correlations.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    for metric, value in corr_dict.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
